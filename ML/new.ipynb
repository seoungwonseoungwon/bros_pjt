{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "521cf9ff-587c-4bb7-b733-7a7953ce6c19",
   "metadata": {},
   "source": [
    "## 신경망 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c92454b-2d97-484f-b5c5-6b34768c2e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 597ms/step - loss: 2.4291 - accuracy: 0.0417 - val_loss: 2.2900 - val_accuracy: 0.1667\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.3943 - accuracy: 0.0417 - val_loss: 2.2862 - val_accuracy: 0.1667\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.3614 - accuracy: 0.0417 - val_loss: 2.2836 - val_accuracy: 0.1667\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.3296 - accuracy: 0.1250 - val_loss: 2.2820 - val_accuracy: 0.1667\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2987 - accuracy: 0.1250 - val_loss: 2.2808 - val_accuracy: 0.1667\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.2692 - accuracy: 0.1250 - val_loss: 2.2797 - val_accuracy: 0.1667\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.2406 - accuracy: 0.1250 - val_loss: 2.2793 - val_accuracy: 0.1667\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.2128 - accuracy: 0.1250 - val_loss: 2.2797 - val_accuracy: 0.1667\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.1857 - accuracy: 0.1250 - val_loss: 2.2806 - val_accuracy: 0.1667\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.1594 - accuracy: 0.2083 - val_loss: 2.2812 - val_accuracy: 0.1667\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.1339 - accuracy: 0.2083 - val_loss: 2.2822 - val_accuracy: 0.1667\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.1096 - accuracy: 0.2083 - val_loss: 2.2837 - val_accuracy: 0.1667\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.0856 - accuracy: 0.2083 - val_loss: 2.2858 - val_accuracy: 0.1667\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.0619 - accuracy: 0.2083 - val_loss: 2.2885 - val_accuracy: 0.1667\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.0387 - accuracy: 0.2083 - val_loss: 2.2918 - val_accuracy: 0.1667\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.0157 - accuracy: 0.2500 - val_loss: 2.2966 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.9935 - accuracy: 0.2917 - val_loss: 2.3023 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.9719 - accuracy: 0.2917 - val_loss: 2.3082 - val_accuracy: 0.1667\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.9505 - accuracy: 0.4167 - val_loss: 2.3142 - val_accuracy: 0.1667\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.9290 - accuracy: 0.4167 - val_loss: 2.3201 - val_accuracy: 0.1667\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.9076 - accuracy: 0.4167 - val_loss: 2.3262 - val_accuracy: 0.1667\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.8865 - accuracy: 0.4167 - val_loss: 2.3321 - val_accuracy: 0.1667\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.8654 - accuracy: 0.4167 - val_loss: 2.3383 - val_accuracy: 0.1667\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.8446 - accuracy: 0.4583 - val_loss: 2.3452 - val_accuracy: 0.1667\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.8239 - accuracy: 0.4583 - val_loss: 2.3527 - val_accuracy: 0.1667\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.8033 - accuracy: 0.4583 - val_loss: 2.3608 - val_accuracy: 0.1667\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.7826 - accuracy: 0.4583 - val_loss: 2.3696 - val_accuracy: 0.1667\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.7619 - accuracy: 0.4583 - val_loss: 2.3782 - val_accuracy: 0.1667\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.7415 - accuracy: 0.4583 - val_loss: 2.3876 - val_accuracy: 0.1667\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.7210 - accuracy: 0.4583 - val_loss: 2.3984 - val_accuracy: 0.1667\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.7005 - accuracy: 0.5417 - val_loss: 2.4103 - val_accuracy: 0.1667\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.6801 - accuracy: 0.5417 - val_loss: 2.4227 - val_accuracy: 0.1667\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.6600 - accuracy: 0.5417 - val_loss: 2.4353 - val_accuracy: 0.1667\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.6401 - accuracy: 0.5417 - val_loss: 2.4489 - val_accuracy: 0.1667\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.6201 - accuracy: 0.5417 - val_loss: 2.4628 - val_accuracy: 0.1667\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.6001 - accuracy: 0.5417 - val_loss: 2.4773 - val_accuracy: 0.1667\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.5805 - accuracy: 0.5417 - val_loss: 2.4930 - val_accuracy: 0.1667\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.5608 - accuracy: 0.5417 - val_loss: 2.5097 - val_accuracy: 0.1667\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.5413 - accuracy: 0.5417 - val_loss: 2.5274 - val_accuracy: 0.1667\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5222 - accuracy: 0.5417 - val_loss: 2.5460 - val_accuracy: 0.1667\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.5032 - accuracy: 0.5417 - val_loss: 2.5648 - val_accuracy: 0.1667\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.4843 - accuracy: 0.5417 - val_loss: 2.5840 - val_accuracy: 0.1667\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.4655 - accuracy: 0.5417 - val_loss: 2.6041 - val_accuracy: 0.1667\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.4469 - accuracy: 0.5417 - val_loss: 2.6255 - val_accuracy: 0.1667\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.4286 - accuracy: 0.5417 - val_loss: 2.6480 - val_accuracy: 0.1667\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.4104 - accuracy: 0.5417 - val_loss: 2.6713 - val_accuracy: 0.1667\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.3923 - accuracy: 0.5417 - val_loss: 2.6954 - val_accuracy: 0.1667\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.3744 - accuracy: 0.5417 - val_loss: 2.7203 - val_accuracy: 0.1667\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.3566 - accuracy: 0.5417 - val_loss: 2.7457 - val_accuracy: 0.1667\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.3392 - accuracy: 0.5417 - val_loss: 2.7722 - val_accuracy: 0.1667\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.3221 - accuracy: 0.5417 - val_loss: 2.8002 - val_accuracy: 0.1667\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.3052 - accuracy: 0.5417 - val_loss: 2.8290 - val_accuracy: 0.1667\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.2885 - accuracy: 0.5417 - val_loss: 2.8581 - val_accuracy: 0.1667\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2719 - accuracy: 0.5417 - val_loss: 2.8874 - val_accuracy: 0.1667\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.2556 - accuracy: 0.5417 - val_loss: 2.9173 - val_accuracy: 0.1667\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.2395 - accuracy: 0.5417 - val_loss: 2.9476 - val_accuracy: 0.1667\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2235 - accuracy: 0.5833 - val_loss: 2.9784 - val_accuracy: 0.1667\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.2081 - accuracy: 0.5833 - val_loss: 3.0097 - val_accuracy: 0.1667\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1927 - accuracy: 0.5833 - val_loss: 3.0414 - val_accuracy: 0.1667\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1778 - accuracy: 0.5833 - val_loss: 3.0736 - val_accuracy: 0.1667\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.1630 - accuracy: 0.5833 - val_loss: 3.1071 - val_accuracy: 0.1667\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.1485 - accuracy: 0.5833 - val_loss: 3.1412 - val_accuracy: 0.1667\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1343 - accuracy: 0.5833 - val_loss: 3.1759 - val_accuracy: 0.1667\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1202 - accuracy: 0.5833 - val_loss: 3.2113 - val_accuracy: 0.1667\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1063 - accuracy: 0.5833 - val_loss: 3.2471 - val_accuracy: 0.1667\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0927 - accuracy: 0.5833 - val_loss: 3.2821 - val_accuracy: 0.1667\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0796 - accuracy: 0.5833 - val_loss: 3.3169 - val_accuracy: 0.1667\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0670 - accuracy: 0.5833 - val_loss: 3.3513 - val_accuracy: 0.1667\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0547 - accuracy: 0.5833 - val_loss: 3.3855 - val_accuracy: 0.1667\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.0427 - accuracy: 0.5833 - val_loss: 3.4196 - val_accuracy: 0.1667\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0311 - accuracy: 0.5833 - val_loss: 3.4538 - val_accuracy: 0.1667\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.0198 - accuracy: 0.5833 - val_loss: 3.4883 - val_accuracy: 0.1667\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0088 - accuracy: 0.5833 - val_loss: 3.5223 - val_accuracy: 0.1667\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9981 - accuracy: 0.5833 - val_loss: 3.5562 - val_accuracy: 0.1667\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9877 - accuracy: 0.5833 - val_loss: 3.5899 - val_accuracy: 0.1667\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9776 - accuracy: 0.5833 - val_loss: 3.6235 - val_accuracy: 0.1667\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.9678 - accuracy: 0.5833 - val_loss: 3.6572 - val_accuracy: 0.1667\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9583 - accuracy: 0.5833 - val_loss: 3.6908 - val_accuracy: 0.1667\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9491 - accuracy: 0.5833 - val_loss: 3.7245 - val_accuracy: 0.1667\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9402 - accuracy: 0.5833 - val_loss: 3.7580 - val_accuracy: 0.1667\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9315 - accuracy: 0.5833 - val_loss: 3.7912 - val_accuracy: 0.1667\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9231 - accuracy: 0.5833 - val_loss: 3.8242 - val_accuracy: 0.1667\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9149 - accuracy: 0.5833 - val_loss: 3.8571 - val_accuracy: 0.1667\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9070 - accuracy: 0.5833 - val_loss: 3.8894 - val_accuracy: 0.1667\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.8994 - accuracy: 0.5833 - val_loss: 3.9213 - val_accuracy: 0.1667\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8920 - accuracy: 0.5833 - val_loss: 3.9526 - val_accuracy: 0.1667\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8848 - accuracy: 0.5833 - val_loss: 3.9838 - val_accuracy: 0.1667\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8779 - accuracy: 0.5833 - val_loss: 4.0155 - val_accuracy: 0.1667\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8712 - accuracy: 0.5833 - val_loss: 4.0473 - val_accuracy: 0.1667\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8647 - accuracy: 0.5833 - val_loss: 4.0789 - val_accuracy: 0.1667\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8584 - accuracy: 0.5833 - val_loss: 4.1101 - val_accuracy: 0.1667\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8523 - accuracy: 0.5833 - val_loss: 4.1411 - val_accuracy: 0.1667\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.8464 - accuracy: 0.5833 - val_loss: 4.1719 - val_accuracy: 0.1667\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8407 - accuracy: 0.5833 - val_loss: 4.2025 - val_accuracy: 0.1667\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8353 - accuracy: 0.5833 - val_loss: 4.2330 - val_accuracy: 0.1667\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.8300 - accuracy: 0.5833 - val_loss: 4.2629 - val_accuracy: 0.1667\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8250 - accuracy: 0.5833 - val_loss: 4.2919 - val_accuracy: 0.1667\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8201 - accuracy: 0.5833 - val_loss: 4.3202 - val_accuracy: 0.1667\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8154 - accuracy: 0.5833 - val_loss: 4.3485 - val_accuracy: 0.1667\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8109 - accuracy: 0.5833 - val_loss: 4.3769 - val_accuracy: 0.1667\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.3769 - accuracy: 0.1667\n",
      "검증 세트 손실: 4.376875400543213\n",
      "검증 세트 정확도: 0.1666666716337204\n",
      "INFO:tensorflow:Assets written to: path_to_model_directory\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: path_to_model_directory\\assets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 훈련 데이터를 불러옵니다.\n",
    "train_data = pd.read_csv(\"KBO_TRAIN.csv\")\n",
    "\n",
    "# 특성(X)과 타겟(y)으로 나눕니다.\n",
    "X_train, y_train = train_data.drop('GRADE', axis=1), train_data['GRADE']\n",
    "\n",
    "# Convert class labels to be in the range from 0 to 9\n",
    "y_train = y_train - 1\n",
    "\n",
    "# 훈련 데이터와 검증 데이터로 나눕니다.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# One-Hot Encoding을 수행하여 팀 이름을 숫자로 변환\n",
    "encoder = OneHotEncoder()\n",
    "X_combined = pd.concat([X_train, X_val])  # Combine the training and validation sets\n",
    "X_encoded = encoder.fit_transform(X_combined[['TEAM']])\n",
    "\n",
    "# 데이터 전처리를 진행합니다.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_encoded[:len(X_train)].toarray())\n",
    "X_val_scaled = scaler.transform(X_encoded[len(X_train):].toarray())\n",
    "\n",
    "# 신경망 모델의 구조를 정의합니다.\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  # KBO 팀 수에 해당하는 10개의 클래스 (0부터 9)\n",
    "])\n",
    "\n",
    "# 모델을 컴파일합니다.\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델을 훈련합니다.\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_val_scaled, y_val))\n",
    "\n",
    "# 검증 세트로 모델을 평가합니다.\n",
    "val_loss, val_accuracy = model.evaluate(X_val_scaled, y_val)\n",
    "print(\"검증 세트 손실:\", val_loss)\n",
    "print(\"검증 세트 정확도:\", val_accuracy)\n",
    "\n",
    "# 훈련된 모델을 파일로 저장합니다.\n",
    "# 훈련된 모델을 파일로 저장합니다.\n",
    "# 훈련된 모델을 TensorFlow's SavedModel format으로 저장합니다.\n",
    "model.save(\"path_to_model_directory\", save_format='tf')\n",
    "model.save(\"path_to_model_directory.h5\", save_format='tf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4367dbc2-f3ef-49e7-a841-5d2aa97eca9a",
   "metadata": {},
   "source": [
    "## 2차 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dbe35951-9758-494f-b65d-b457b6bddee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000026DD25F1A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000026DD25F1A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "   SEASON TEAM    AVG    OBP    SLG   ERA  WHIP  GRADE  PREDICTED_GRADE\n",
      "0      23   LG  0.285  0.373  0.394  3.66  1.33      1                3\n",
      "1      23   NC  0.266  0.346  0.384  3.67  1.34      4                6\n",
      "2      23  SSG  0.259  0.336  0.396  4.06  1.50      2                1\n",
      "3      23   두산  0.257  0.336  0.372  3.90  1.36      3                3\n",
      "4      23  KIA  0.260  0.331  0.367  3.82  1.44      7                9\n",
      "5      23   키움  0.254  0.328  0.347  3.77  1.33      9                5\n",
      "6      23   KT  0.264  0.335  0.360  4.38  1.40      6                3\n",
      "7      23   한화  0.241  0.328  0.344  3.88  1.34      8               10\n",
      "8      23   삼성  0.252  0.322  0.356  4.57  1.46     10                8\n",
      "9      23   롯데  0.255  0.327  0.346  4.53  1.52      5                8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 훈련된 모델을 불러옵니다.\n",
    "model = joblib.load(\"path_to_model_file.pkl\")\n",
    "\n",
    "# 테스트 데이터를 불러옵니다.\n",
    "test_data = pd.read_csv(\"KBO_TEST.csv\")\n",
    "\n",
    "# 특성(X)과 타겟(y)으로 나눕니다. 테스트 데이터에는 'GRADE' 열이 없으므로 예측 결과를 추가할 열로 사용합니다.\n",
    "X_test = test_data.drop('GRADE', axis=1)\n",
    "y_test = test_data['GRADE']\n",
    "\n",
    "# One-Hot Encoding을 수행하여 'TEAM' 열을 숫자로 변환합니다.\n",
    "encoder = OneHotEncoder()\n",
    "X_combined = pd.concat([X_train, X_val, X_test])  # 훈련, 검증, 테스트 데이터를 합쳐서 One-Hot Encoding을 적용합니다.\n",
    "X_encoded = encoder.fit_transform(X_combined[['TEAM']])\n",
    "X_test_encoded = X_encoded[len(X_train) + len(X_val):]\n",
    "\n",
    "# 데이터 전처리를 진행합니다. 표준화를 위해 StandardScaler를 사용합니다.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_encoded[:len(X_train)].toarray())\n",
    "X_val_scaled = scaler.transform(X_encoded[len(X_train):len(X_train) + len(X_val)].toarray())\n",
    "X_test_scaled = scaler.transform(X_test_encoded.toarray())\n",
    "\n",
    "# 테스트 데이터에 대해 예측을 수행합니다.\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# 예측 결과는 각 클래스(0부터 9)에 대한 확률값입니다. 예측된 클래스를 구합니다.\n",
    "predicted_classes = predictions.argmax(axis=1)\n",
    "\n",
    "# 예측된 클래스 레이블을 원래 범위로 변환합니다 (1부터 10).\n",
    "predicted_classes = predicted_classes + 1\n",
    "\n",
    "# 예측된 \"GRADE\" 열을 테스트 데이터 DataFrame에 추가합니다.\n",
    "test_data['PREDICTED_GRADE'] = predicted_classes\n",
    "\n",
    "# 예측 결과가 추가된 test_data DataFrame을 출력합니다.\n",
    "print(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3d3cde1-6db9-4891-a773-3a19c9f9cc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "LG 팀의 예상 등수 및 확률 Top 3:\n",
      "   1위: 3 (확률: 0.6221)\n",
      "   2위: None (확률: None)\n",
      "   3위: None (확률: None)\n",
      "NC 팀의 예상 등수 및 확률 Top 3:\n",
      "   1위: 6 (확률: 0.3016)\n",
      "   2위: None (확률: None)\n",
      "   3위: None (확률: None)\n",
      "SSG 팀의 예상 등수 및 확률 Top 3:\n",
      "   1위: 1 (확률: 0.3220)\n",
      "   2위: None (확률: None)\n",
      "   3위: None (확률: None)\n",
      "두산 팀의 예상 등수 및 확률 Top 3:\n",
      "   1위: 3 (확률: 0.4724)\n",
      "   2위: None (확률: None)\n",
      "   3위: None (확률: None)\n",
      "KIA 팀의 예상 등수 및 확률 Top 3:\n",
      "   1위: 5 (확률: 0.4511)\n",
      "   2위: None (확률: None)\n",
      "   3위: None (확률: None)\n",
      "키움 팀의 예상 등수 및 확률 Top 3:\n",
      "   1위: 5 (확률: 0.9514)\n",
      "   2위: None (확률: None)\n",
      "   3위: None (확률: None)\n",
      "KT 팀의 예상 등수 및 확률 Top 3:\n",
      "   1위: 3 (확률: 0.2663)\n",
      "   2위: None (확률: None)\n",
      "   3위: None (확률: None)\n",
      "한화 팀의 예상 등수 및 확률 Top 3:\n",
      "   1위: 10 (확률: 0.9181)\n",
      "   2위: None (확률: None)\n",
      "   3위: None (확률: None)\n",
      "삼성 팀의 예상 등수 및 확률 Top 3:\n",
      "   1위: 8 (확률: 0.9393)\n",
      "   2위: None (확률: None)\n",
      "   3위: None (확률: None)\n",
      "롯데 팀의 예상 등수 및 확률 Top 3:\n",
      "   1위: 8 (확률: 0.5527)\n",
      "   2위: None (확률: None)\n",
      "   3위: None (확률: None)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 훈련된 모델을 불러옵니다.\n",
    "model = keras.models.load_model(\"path_to_model_directory.h5\")\n",
    "\n",
    "# 테스트 데이터를 불러옵니다.\n",
    "test_data = pd.read_csv(\"KBO_TEST.csv\")\n",
    "\n",
    "# 특성(X)과 타겟(y)으로 나눕니다. 테스트 데이터에는 'GRADE' 열이 없으므로 예측 결과를 추가할 열로 사용합니다.\n",
    "X_test = test_data.drop('GRADE', axis=1)\n",
    "y_test = test_data['GRADE']\n",
    "\n",
    "# One-Hot Encoding을 수행하여 'TEAM' 열을 숫자로 변환합니다.\n",
    "encoder = OneHotEncoder()\n",
    "X_combined = pd.concat([X_train, X_val, X_test])  # 훈련, 검증, 테스트 데이터를 합쳐서 One-Hot Encoding을 적용합니다.\n",
    "X_encoded = encoder.fit_transform(X_combined[['TEAM']])\n",
    "X_test_encoded = X_encoded[len(X_train) + len(X_val):]\n",
    "\n",
    "# 데이터 전처리를 진행합니다. 표준화를 위해 StandardScaler를 사용합니다.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_encoded[:len(X_train)].toarray())\n",
    "X_val_scaled = scaler.transform(X_encoded[len(X_train):len(X_train) + len(X_val)].toarray())\n",
    "X_test_scaled = scaler.transform(X_test_encoded.toarray())\n",
    "\n",
    "# 테스트 데이터에 대해 예측을 수행합니다.\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# 예측 결과는 각 클래스(0부터 9)에 대한 확률값입니다. 예측된 클래스를 구합니다.\n",
    "predicted_classes = predictions.argmax(axis=1)\n",
    "\n",
    "# 예측된 클래스 레이블을 원래 범위로 변환합니다 (1부터 10).\n",
    "predicted_classes = predicted_classes + 1\n",
    "\n",
    "# 예측된 \"GRADE\" 열을 테스트 데이터 DataFrame에 추가합니다.\n",
    "test_data['PREDICTED_GRADE'] = predicted_classes\n",
    "\n",
    "# 각 팀별로 예상 등수와 확률의 Top 3를 뽑아냅니다.\n",
    "team_predictions = {}\n",
    "for team, pred in zip(test_data['TEAM'], predictions):\n",
    "    if team not in team_predictions:\n",
    "        team_predictions[team] = []\n",
    "    team_predictions[team].append((np.argmax(pred) + 1, np.max(pred)))\n",
    "\n",
    "# 각 팀별로 예상 등수와 확률의 Top 3를 구합니다. (grade, prob)\n",
    "# 각 팀별로 예상 등수와 확률의 Top 3를 구합니다. (grade, prob)\n",
    "def get_top3(predictions):\n",
    "    top3 = sorted(predictions, key=lambda x: x[1], reverse=True)[:3]\n",
    "    if len(top3) < 3:\n",
    "        top3.extend([(None, None)] * (3 - len(top3)))\n",
    "    return top3\n",
    "\n",
    "top3_teams = {team: get_top3(preds) for team, preds in team_predictions.items()}\n",
    "\n",
    "# Top 3 결과를 출력합니다.\n",
    "for team, top3 in top3_teams.items():\n",
    "    print(f\"{team} 팀의 예상 등수 및 확률 Top 3:\")\n",
    "    for rank, (grade, prob) in enumerate(top3, 1):\n",
    "        grade_str = str(grade) if grade else \"None\"\n",
    "        prob_str = f\"{prob:.4f}\" if prob is not None else \"None\"\n",
    "        print(f\"   {rank}위: {grade_str} (확률: {prob_str})\" if grade_str != \"None\" else f\"   {rank}위: {grade_str} (확률: {prob_str})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "579e1bed-b104-476c-af1c-c92ad1e44e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEASON</th>\n",
       "      <th>TEAM</th>\n",
       "      <th>AVG</th>\n",
       "      <th>OBP</th>\n",
       "      <th>SLG</th>\n",
       "      <th>ERA</th>\n",
       "      <th>WHIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>22</td>\n",
       "      <td>두산</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.365</td>\n",
       "      <td>4.46</td>\n",
       "      <td>1.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>22</td>\n",
       "      <td>KIA</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.398</td>\n",
       "      <td>4.21</td>\n",
       "      <td>1.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21</td>\n",
       "      <td>한화</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.341</td>\n",
       "      <td>4.68</td>\n",
       "      <td>1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.428</td>\n",
       "      <td>4.39</td>\n",
       "      <td>1.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>삼성</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.394</td>\n",
       "      <td>4.78</td>\n",
       "      <td>1.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SEASON TEAM    AVG    OBP    SLG   ERA  WHIP\n",
       "28      22   두산  0.255  0.324  0.365  4.46  1.48\n",
       "24      22  KIA  0.272  0.349  0.398  4.21  1.42\n",
       "12      21   한화  0.237  0.334  0.341  4.68  1.49\n",
       "0       20   LG  0.277  0.349  0.428  4.39  1.42\n",
       "4       20   삼성  0.268  0.338  0.394  4.78  1.47"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dde260e1-53cd-4067-a84d-fc8709b1f74b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "72",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 훈련된 모델을 불러옵니다.\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath_to_model_file.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 테스트 데이터를 불러옵니다.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKBO_TEST.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\playdata_lab\\team\\team\\lib\\site-packages\\joblib\\numpy_pickle.py:658\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[0;32m    656\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[1;32m--> 658\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mC:\\playdata_lab\\team\\team\\lib\\site-packages\\joblib\\numpy_pickle.py:577\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    575\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[0;32m    579\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[0;32m    583\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1212\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[1;31mKeyError\u001b[0m: 72"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "# 훈련된 모델을 불러옵니다.\n",
    "model = joblib.load(\"path_to_model_file.h5\")\n",
    "\n",
    "# 테스트 데이터를 불러옵니다.\n",
    "test_data = pd.read_csv(\"KBO_TEST.csv\")\n",
    "\n",
    "# 특성(X)과 타겟(y)으로 나눕니다. 테스트 데이터에는 'GRADE' 열이 없으므로 예측 결과를 추가할 열로 사용합니다.\n",
    "X_test = test_data.drop('GRADE', axis=1)\n",
    "y_test = test_data['GRADE']\n",
    "\n",
    "# One-Hot Encoding을 수행하여 'TEAM' 열을 숫자로 변환합니다.\n",
    "encoder = OneHotEncoder()\n",
    "X_combined = pd.concat([X_train, X_val, X_test])  # 훈련, 검증, 테스트 데이터를 합쳐서 One-Hot Encoding을 적용합니다.\n",
    "X_encoded = encoder.fit_transform(X_combined[['TEAM']])\n",
    "X_test_encoded = X_encoded[len(X_train) + len(X_val):]\n",
    "\n",
    "# 데이터 전처리를 진행합니다. 표준화를 위해 StandardScaler를 사용합니다.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_encoded[:len(X_train)].toarray())\n",
    "X_val_scaled = scaler.transform(X_encoded[len(X_train):len(X_train) + len(X_val)].toarray())\n",
    "X_test_scaled = scaler.transform(X_test_encoded.toarray())\n",
    "\n",
    "# 테스트 데이터에 대해 예측을 수행합니다.\n",
    "@tf.function\n",
    "def predict(X):\n",
    "    return model.predict(X)\n",
    "\n",
    "predictions = predict(X_test_scaled)\n",
    "\n",
    "# 예측 결과는 각 클래스(0부터 9)에 대한 확률값입니다. 예측된 클래스를 구합니다.\n",
    "predicted_classes = predictions.argmax(axis=1)\n",
    "\n",
    "# 예측된 클래스 레이블을 원래 범위로 변환합니다 (1부터 10).\n",
    "predicted_classes = predicted_classes + 1\n",
    "\n",
    "# 예측된 \"GRADE\" 열을 테스트 데이터 DataFrame에 추가합니다.\n",
    "test_data['PREDICTED_GRADE'] = predicted_classes\n",
    "\n",
    "# 각 팀별로 예상 등수와 확률의 Top 3를 뽑아냅니다.\n",
    "team_predictions = test_data.groupby('TEAM')['PREDICTED_GRADE'].apply(list).to_dict()\n",
    "\n",
    "# 각 팀별로 예상 등수와 확률의 Top 3를 구합니다. (grade, prob)\n",
    "def get_top3(predictions):\n",
    "    top3 = sorted(predictions, key=lambda x: x[1], reverse=True)[:3]\n",
    "    if len(top3) < 3:\n",
    "        top3.extend([(None, None)] * (3 - len(top3)))\n",
    "    return top3\n",
    "\n",
    "top3_teams = {team: get_top3(preds) for team, preds in team_predictions.items()}\n",
    "\n",
    "# Top 3 결과를 출력합니다.\n",
    "for team, top3 in top3_teams.items():\n",
    "    print(f\"{team} 팀의 예상 등수 및 확률 Top 3:\")\n",
    "    for rank, (grade, prob) in enumerate(top3, 1):\n",
    "        grade_str = str(grade) if grade else \"None\"\n",
    "        prob_str = f\"{prob:.4f}\" if prob is not None else \"None\"\n",
    "        print(f\"   {rank}위: {grade_str} (확률: {prob_str})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b41735-be8a-4f83-a9d7-40fa377fe211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 예측 결과:\n",
      "[ 2  2  2  2  2 10  2  1  5  2  3  4 10  2  2  2  2  2  5  5  4  3  3  3\n",
      "  2  3 10  2 10  2]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# KBO_TRAIN.csv 파일 로드\n",
    "train_data = pd.read_csv(\"KBO_TRAIN.csv\")\n",
    "\n",
    "# 데이터 전처리\n",
    "X_train = train_data.drop(['SEASON', 'TEAM', 'GRADE'], axis=1)\n",
    "y_train = train_data['GRADE']\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# 머신러닝 모델 학습\n",
    "model = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 테스트 데이터 로드\n",
    "test_data = pd.read_csv(\"KBO_TEST.csv\")\n",
    "\n",
    "# 데이터 전처리\n",
    "X_test = test_data.drop(['SEASON', 'TEAM', 'GRADE'], axis=1)\n",
    "y_test = test_data['GRADE']\n",
    "\n",
    "# 스케일링\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 테스트 데이터로 예측\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(\"테스트 데이터 예측 결과:\")\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bddc92db-2dd6-4b47-85cd-dfcb132f09e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_model_file.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_test)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Load the pre-trained neural network model\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath_to_model_file.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace \"path_to_model_file.pkl\" with your actual model file path\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     23\u001b[0m pred_top3, pred_detail \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n",
      "File \u001b[1;32mC:\\playdata_lab\\team\\team\\lib\\site-packages\\joblib\\numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_model_file.pkl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tabulate import tabulate\n",
    "import joblib\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv(\"KBO_TEST.csv\")\n",
    "\n",
    "# Preprocess X_test\n",
    "X_test = test_data.drop('GRADE', axis=1)\n",
    "X_test = X_test.drop('SEASON', axis=1)  # Drop the SEASON column if it's not needed\n",
    "X_test = X_test.drop('TEAM', axis=1)  # Drop the TEAM column if it's not needed\n",
    "\n",
    "# Standard Scaling (if needed)\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "# Load the pre-trained neural network model\n",
    "model = joblib.load(\"path_to_model_file.pkl\")  # Replace \"path_to_model_file.pkl\" with your actual model file path\n",
    "\n",
    "# Make predictions\n",
    "pred_top3, pred_detail = model.predict(X_test_scaled)\n",
    "\n",
    "# Print the results using tabulate\n",
    "print(tabulate(pred_top3, headers='keys', tablefmt='psql'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79df79a-fec0-4d5a-81e9-e168a56766f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "team",
   "language": "python",
   "name": "team"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
